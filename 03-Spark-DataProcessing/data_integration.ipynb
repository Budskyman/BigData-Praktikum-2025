{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T7nOEGxGMlb-",
        "outputId": "fe71892f-7d69-4d86-fb4f-3dfeadfedd6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== SETUP DATA INTEGRATION SERVICES ===\n",
            "Services yang berjalan:\n",
            "  MySQL: ‚úÖ Running (port 3306) - Sumber data terstruktur\n",
            "  Kafka: ‚úÖ Running (port 9092) - Message streaming\n",
            "  Zookeeper: ‚úÖ Running (port 2181) - Koordinasi Kafka\n"
          ]
        }
      ],
      "source": [
        "# Simulasi Docker Compose untuk Data Integration\n",
        "print(\"=== SETUP DATA INTEGRATION SERVICES ===\")\n",
        "\n",
        "services = {\n",
        "    \"MySQL\": {\"port\": 3306, \"status\": \"‚úÖ Running\", \"purpose\": \"Sumber data terstruktur\"},\n",
        "    \"Kafka\": {\"port\": 9092, \"status\": \"‚úÖ Running\", \"purpose\": \"Message streaming\"},\n",
        "    \"Zookeeper\": {\"port\": 2181, \"status\": \"‚úÖ Running\", \"purpose\": \"Koordinasi Kafka\"}\n",
        "}\n",
        "\n",
        "print(\"Services yang berjalan:\")\n",
        "for service, info in services.items():\n",
        "    print(f\"  {service}: {info['status']} (port {info['port']}) - {info['purpose']}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === SQOOP: TRANSFER DATA MYSQL -> HDFS ===\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"PRAKTIKUM 1: APACHE SQOOP\")\n",
        "print(\"Transfer data dari MySQL ke HDFS\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Simulasi data dari MySQL database\n",
        "print(\"üìä STEP 1: Data di MySQL Database 'company'\")\n",
        "mysql_employees = [\n",
        "    (1, 'Andi', 'Engineering', 7500000),\n",
        "    (2, 'Budi', 'Marketing', 6500000),\n",
        "    (3, 'Citra', 'Engineering', 8000000),\n",
        "    (4, 'Dewi', 'HR', 6000000),\n",
        "    (5, 'Eka', 'Engineering', 9000000)\n",
        "]\n",
        "\n",
        "import pandas as pd\n",
        "df_mysql = pd.DataFrame(mysql_employees, columns=['id', 'nama', 'department', 'gaji'])\n",
        "print(\"Tabel 'employees' di MySQL:\")\n",
        "print(df_mysql)\n",
        "\n",
        "# Simulasi Sqoop Import\n",
        "print(\"\\nüì• STEP 2: Sqoop Import ke HDFS\")\n",
        "print(\"Perintah: sqoop import --connect jdbc:mysql://localhost/company --table employees --target-dir /user/hadoop/employees -m 1\")\n",
        "\n",
        "# Simulasi data di HDFS setelah import\n",
        "print(\"‚úÖ Data berhasil diimport ke HDFS: /user/hadoop/employees/\")\n",
        "hdfs_data = df_mysql.copy()\n",
        "print(\"Isi HDFS setelah import:\")\n",
        "print(hdfs_data)\n",
        "\n",
        "# Simulasi Sqoop Export\n",
        "print(\"\\nüì§ STEP 3: Sqoop Export dari HDFS ke MySQL\")\n",
        "print(\"Buat data baru di HDFS:\")\n",
        "new_data = pd.DataFrame([(6, 'Fajar', 'Sales', 7000000)], columns=['id', 'nama', 'department', 'gaji'])\n",
        "print(new_data)\n",
        "\n",
        "print(\"Perintah: sqoop export --connect jdbc:mysql://localhost/company --table employees_export --export-dir /user/hadoop/new_employees\")\n",
        "print(\"‚úÖ Data berhasil diexport ke MySQL tabel 'employees_export'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYCh6rNpMqTC",
        "outputId": "318f6358-365a-4a59-dee8-e6598fe09b9a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "PRAKTIKUM 1: APACHE SQOOP\n",
            "Transfer data dari MySQL ke HDFS\n",
            "==================================================\n",
            "üìä STEP 1: Data di MySQL Database 'company'\n",
            "Tabel 'employees' di MySQL:\n",
            "   id   nama   department     gaji\n",
            "0   1   Andi  Engineering  7500000\n",
            "1   2   Budi    Marketing  6500000\n",
            "2   3  Citra  Engineering  8000000\n",
            "3   4   Dewi           HR  6000000\n",
            "4   5    Eka  Engineering  9000000\n",
            "\n",
            "üì• STEP 2: Sqoop Import ke HDFS\n",
            "Perintah: sqoop import --connect jdbc:mysql://localhost/company --table employees --target-dir /user/hadoop/employees -m 1\n",
            "‚úÖ Data berhasil diimport ke HDFS: /user/hadoop/employees/\n",
            "Isi HDFS setelah import:\n",
            "   id   nama   department     gaji\n",
            "0   1   Andi  Engineering  7500000\n",
            "1   2   Budi    Marketing  6500000\n",
            "2   3  Citra  Engineering  8000000\n",
            "3   4   Dewi           HR  6000000\n",
            "4   5    Eka  Engineering  9000000\n",
            "\n",
            "üì§ STEP 3: Sqoop Export dari HDFS ke MySQL\n",
            "Buat data baru di HDFS:\n",
            "   id   nama department     gaji\n",
            "0   6  Fajar      Sales  7000000\n",
            "Perintah: sqoop export --connect jdbc:mysql://localhost/company --table employees_export --export-dir /user/hadoop/new_employees\n",
            "‚úÖ Data berhasil diexport ke MySQL tabel 'employees_export'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === FLUME: LOG DATA COLLECTION ===\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"PRAKTIKUM 2: APACHE FLUME\")\n",
        "print(\"Koleksi data log real-time\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "print(\"üîÑ STEP 1: Konfigurasi Flume Agent\")\n",
        "flume_config = \"\"\"\n",
        "# Agent components\n",
        "a1.sources = r1\n",
        "a1.sinks = k1\n",
        "a1.channels = c1\n",
        "\n",
        "# NetCat Source\n",
        "a1.sources.r1.type = netcat\n",
        "a1.sources.r1.bind = localhost\n",
        "a1.sources.r1.port = 44444\n",
        "\n",
        "# Logger Sink\n",
        "a1.sinks.k1.type = logger\n",
        "\n",
        "# Memory Channel\n",
        "a1.channels.c1.type = memory\n",
        "a1.channels.c1.capacity = 1000\n",
        "\n",
        "# Binding\n",
        "a1.sources.r1.channels = c1\n",
        "a1.sinks.k1.channel = c1\n",
        "\"\"\"\n",
        "\n",
        "print(\"File: netcat-logger.conf\")\n",
        "print(flume_config)\n",
        "\n",
        "print(\"\\nüì® STEP 2: Simulasi Data Streaming ke Flume\")\n",
        "import time\n",
        "\n",
        "logs = [\n",
        "    \"2024-01-15 10:30:15 INFO: User login successful - user_id: 12345\",\n",
        "    \"2024-01-15 10:31:22 ERROR: Database connection timeout - retrying...\",\n",
        "    \"2024-01-15 10:32:45 INFO: Data processing completed - records: 1000\",\n",
        "    \"2024-01-15 10:33:10 WARN: High memory usage detected - 85%\",\n",
        "    \"2024-01-15 10:34:30 INFO: Backup job started - size: 2.5GB\"\n",
        "]\n",
        "\n",
        "print(\"Flume Agent menerima data log:\")\n",
        "for i, log in enumerate(logs, 1):\n",
        "    print(f\"  [{i}] {log}\")\n",
        "    time.sleep(1)\n",
        "\n",
        "print(\"\\n‚úÖ Flume berhasil mengumpulkan 5 log events\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OyPibeIvMuwK",
        "outputId": "92fb1619-c923-478d-b68e-9587d3e006e1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "PRAKTIKUM 2: APACHE FLUME\n",
            "Koleksi data log real-time\n",
            "==================================================\n",
            "üîÑ STEP 1: Konfigurasi Flume Agent\n",
            "File: netcat-logger.conf\n",
            "\n",
            "# Agent components\n",
            "a1.sources = r1\n",
            "a1.sinks = k1\n",
            "a1.channels = c1\n",
            "\n",
            "# NetCat Source\n",
            "a1.sources.r1.type = netcat\n",
            "a1.sources.r1.bind = localhost\n",
            "a1.sources.r1.port = 44444\n",
            "\n",
            "# Logger Sink\n",
            "a1.sinks.k1.type = logger\n",
            "\n",
            "# Memory Channel\n",
            "a1.channels.c1.type = memory\n",
            "a1.channels.c1.capacity = 1000\n",
            "\n",
            "# Binding\n",
            "a1.sources.r1.channels = c1\n",
            "a1.sinks.k1.channel = c1\n",
            "\n",
            "\n",
            "üì® STEP 2: Simulasi Data Streaming ke Flume\n",
            "Flume Agent menerima data log:\n",
            "  [1] 2024-01-15 10:30:15 INFO: User login successful - user_id: 12345\n",
            "  [2] 2024-01-15 10:31:22 ERROR: Database connection timeout - retrying...\n",
            "  [3] 2024-01-15 10:32:45 INFO: Data processing completed - records: 1000\n",
            "  [4] 2024-01-15 10:33:10 WARN: High memory usage detected - 85%\n",
            "  [5] 2024-01-15 10:34:30 INFO: Backup job started - size: 2.5GB\n",
            "\n",
            "‚úÖ Flume berhasil mengumpulkan 5 log events\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === KAFKA: MESSAGE STREAMING ===\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"PRAKTIKUM 3: APACHE KAFKA\")\n",
        "print(\"Message publishing & subscribing\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "print(\"üìù STEP 1: Buat Kafka Topic\")\n",
        "print(\"Perintah: kafka-topics.sh --create --topic praktikum-bigdata --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1\")\n",
        "print(\"‚úÖ Topic 'praktikum-bigdata' berhasil dibuat\")\n",
        "\n",
        "print(\"\\nüì§ STEP 2: Kafka Producer - Mengirim Pesan\")\n",
        "messages = [\n",
        "    \"Pesan pertama: Hello Kafka!\",\n",
        "    \"Data sensor: temperature=25.6C, humidity=60%\",\n",
        "    \"User action: button_clicked, item_id=789\",\n",
        "    \"System alert: CPU usage 75%\",\n",
        "    \"Transaction: order_id=456, amount=250000\"\n",
        "]\n",
        "\n",
        "print(\"Producer mengirim pesan ke topic 'praktikum-bigdata':\")\n",
        "for i, msg in enumerate(messages, 1):\n",
        "    print(f\"  PRODUCER ü°í [{i}] {msg}\")\n",
        "    time.sleep(1)\n",
        "\n",
        "print(\"\\nüì• STEP 3: Kafka Consumer - Menerima Pesan\")\n",
        "print(\"Perintah: kafka-console-consumer.sh --topic praktikum-bigdata --from-beginning --bootstrap-server localhost:9092\")\n",
        "print(\"Consumer membaca pesan:\")\n",
        "for i, msg in enumerate(messages, 1):\n",
        "    print(f\"  CONSUMER ü°ê [{i}] {msg}\")\n",
        "    time.sleep(1)\n",
        "\n",
        "print(\"\\nüîÑ STEP 4: Real-time Streaming Simulation\")\n",
        "print(\"Producer mengirim pesan baru...\")\n",
        "new_messages = [\n",
        "    \"Real-time update: stock_price=15200\",\n",
        "    \"New user registered: user_id=999\"\n",
        "]\n",
        "\n",
        "for msg in new_messages:\n",
        "    print(f\"  LIVE ü°í {msg}\")\n",
        "    time.sleep(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQ6NvDBGMzmq",
        "outputId": "2cb45df6-1e3c-4c93-e5c2-0efa92b21997"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "PRAKTIKUM 3: APACHE KAFKA\n",
            "Message publishing & subscribing\n",
            "==================================================\n",
            "üìù STEP 1: Buat Kafka Topic\n",
            "Perintah: kafka-topics.sh --create --topic praktikum-bigdata --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1\n",
            "‚úÖ Topic 'praktikum-bigdata' berhasil dibuat\n",
            "\n",
            "üì§ STEP 2: Kafka Producer - Mengirim Pesan\n",
            "Producer mengirim pesan ke topic 'praktikum-bigdata':\n",
            "  PRODUCER ü°í [1] Pesan pertama: Hello Kafka!\n",
            "  PRODUCER ü°í [2] Data sensor: temperature=25.6C, humidity=60%\n",
            "  PRODUCER ü°í [3] User action: button_clicked, item_id=789\n",
            "  PRODUCER ü°í [4] System alert: CPU usage 75%\n",
            "  PRODUCER ü°í [5] Transaction: order_id=456, amount=250000\n",
            "\n",
            "üì• STEP 3: Kafka Consumer - Menerima Pesan\n",
            "Perintah: kafka-console-consumer.sh --topic praktikum-bigdata --from-beginning --bootstrap-server localhost:9092\n",
            "Consumer membaca pesan:\n",
            "  CONSUMER ü°ê [1] Pesan pertama: Hello Kafka!\n",
            "  CONSUMER ü°ê [2] Data sensor: temperature=25.6C, humidity=60%\n",
            "  CONSUMER ü°ê [3] User action: button_clicked, item_id=789\n",
            "  CONSUMER ü°ê [4] System alert: CPU usage 75%\n",
            "  CONSUMER ü°ê [5] Transaction: order_id=456, amount=250000\n",
            "\n",
            "üîÑ STEP 4: Real-time Streaming Simulation\n",
            "Producer mengirim pesan baru...\n",
            "  LIVE ü°í Real-time update: stock_price=15200\n",
            "  LIVE ü°í New user registered: user_id=999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === INTEGRASI SEMUA DATA INTEGRATION TOOLS ===\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"INTEGRASI: DATA PIPELINE LENGKAP\")\n",
        "print(\"MySQL ü°í Sqoop ü°í HDFS ü°í Kafka ü°í Flume ü°í Spark\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"üöÄ ALUR DATA INTEGRATION:\")\n",
        "\n",
        "steps = [\n",
        "    \"1. üìä Data tersimpan di MySQL (transaksi, user data)\",\n",
        "    \"2. üì• Sqoop import data ke HDFS untuk processing\",\n",
        "    \"3. üóÇÔ∏è Data di HDFS diproses oleh Spark/MapReduce\",\n",
        "    \"4. üì§ Hasil processing dikirim ke Kafka topic\",\n",
        "    \"5. üì® Kafka stream data ke berbagai consumers\",\n",
        "    \"6. üìù Flume mengumpulkan application logs\",\n",
        "    \"7. üîÑ Real-time monitoring dan analytics\"\n",
        "]\n",
        "\n",
        "for step in steps:\n",
        "    print(f\"   {step}\")\n",
        "    time.sleep(1)\n",
        "\n",
        "print(\"\\n\" + \"üéØ USE CASE: E-COMMERCE DATA PLATFORM\")\n",
        "print(\"   ‚Ä¢ MySQL: Produk, user, transaksi data\")\n",
        "print(\"   ‚Ä¢ Sqoop: Periodic sync ke data warehouse\")\n",
        "print(\"   ‚Ä¢ Kafka: Real-time user activity streaming\")\n",
        "print(\"   ‚Ä¢ Flume: Application log collection\")\n",
        "print(\"   ‚Ä¢ Spark: Analytics & machine learning\")\n",
        "\n",
        "print(\"\\n‚úÖ DATA INTEGRATION PRAKTIKUM SELESAI!\")\n",
        "print(\"üìä Total komponen yang dipraktikumkan:\")\n",
        "print(\"   ‚Ä¢ Sqoop: Import/Export MySQL-HDFS\")\n",
        "print(\"   ‚Ä¢ Flume: Log collection dengan NetCat source\")\n",
        "print(\"   ‚Ä¢ Kafka: Producer/Consumer messaging\")\n",
        "print(\"   ‚Ä¢ Integrasi: Full data pipeline simulation\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCDW1USPM0T6",
        "outputId": "2fb381a6-c971-483e-fcf2-2785dafcc27c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "INTEGRASI: DATA PIPELINE LENGKAP\n",
            "MySQL ü°í Sqoop ü°í HDFS ü°í Kafka ü°í Flume ü°í Spark\n",
            "============================================================\n",
            "üöÄ ALUR DATA INTEGRATION:\n",
            "   1. üìä Data tersimpan di MySQL (transaksi, user data)\n",
            "   2. üì• Sqoop import data ke HDFS untuk processing\n",
            "   3. üóÇÔ∏è Data di HDFS diproses oleh Spark/MapReduce\n",
            "   4. üì§ Hasil processing dikirim ke Kafka topic\n",
            "   5. üì® Kafka stream data ke berbagai consumers\n",
            "   6. üìù Flume mengumpulkan application logs\n",
            "   7. üîÑ Real-time monitoring dan analytics\n",
            "\n",
            "üéØ USE CASE: E-COMMERCE DATA PLATFORM\n",
            "   ‚Ä¢ MySQL: Produk, user, transaksi data\n",
            "   ‚Ä¢ Sqoop: Periodic sync ke data warehouse\n",
            "   ‚Ä¢ Kafka: Real-time user activity streaming\n",
            "   ‚Ä¢ Flume: Application log collection\n",
            "   ‚Ä¢ Spark: Analytics & machine learning\n",
            "\n",
            "‚úÖ DATA INTEGRATION PRAKTIKUM SELESAI!\n",
            "üìä Total komponen yang dipraktikumkan:\n",
            "   ‚Ä¢ Sqoop: Import/Export MySQL-HDFS\n",
            "   ‚Ä¢ Flume: Log collection dengan NetCat source\n",
            "   ‚Ä¢ Kafka: Producer/Consumer messaging\n",
            "   ‚Ä¢ Integrasi: Full data pipeline simulation\n"
          ]
        }
      ]
    }
  ]
}